import pandas as pd
import numpy as np
import random
import joblib
import tkinter as tk
from tkinter import messagebox, simpledialog, filedialog

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


def df_trim(df):
    out = df.copy()

    # remove rows whose RESULT is in this list
    out = out[~out['RESULT'].isin(['Timeout', 'Penalty', 'Interception'])]

    # drop rows with missing OFF FORM or PERSONNEL
    out = out[out['OFF FORM'].notna() & out['PERSONNEL'].notna()]

    return out

def create_features(df):
    df_out = df.copy()
    df_out['GN/LS LAG 1'] = df_out['GN/LS'].shift(1)
    df_out['GN/LS LAG 2'] = df_out['GN/LS'].shift(2)

    df_out['PLAY # LAG 1'] = df_out['PLAY #'].shift(1)
    df_out['PLAY # LAG 2'] = df_out['PLAY #'].shift(2)

    df_out['PLAY TYPE LAG 1'] = df_out['PLAY TYPE'].shift(1)
    df_out['PLAY TYPE LAG 2'] = df_out['PLAY TYPE'].shift(2)

    df_out.loc[df_out['PLAY #'] - df_out['PLAY # LAG 1'] != 1, 'GN/LS LAG 1'] = 0
    df_out.loc[df_out['PLAY #'] - df_out['PLAY # LAG 2'] != 2, 'GN/LS LAG 2'] = 0

    df_out['OFF FORM LAG 1'] = df_out['OFF FORM'].shift(1)
    df_out['OFF FORM LAG 2'] = df_out['OFF FORM'].shift(2)

    df_out.loc[df_out['PLAY #'] - df_out['PLAY # LAG 1'] != 1, 'OFF FORM LAG 1'] = 'NONE'
    df_out.loc[df_out['PLAY #'] - df_out['PLAY # LAG 2'] != 2, 'OFF FORM LAG 2'] = 'NONE'

    df_out['TIME TO HALF'] = (df_out['TIME TO HALF'].astype(str).str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1])))

    df_out['SCORE DIFF'] = df_out['OPP SCORE'] - df_out['OWN SCORE']

    df_out['HASH'] = df_out['HASH'].astype(str).str.strip()

    df_out['MID OR NOT'] = df_out['HASH'] == 'M'
    df_out['HASH OR NOT'] = df_out['HASH'] != 'M'

    df_out['0-2'] = df_out['DIST'] <= 2
    df_out['2-6'] = (df_out['DIST'] > 2) & (df_out['DIST'] <= 6)
    df_out['6+'] = df_out['DIST'] > 6

    # df['PERSONNEL!'] = df['PERSONNEL'].astype(str).str.strip()
    # df = pd.get_dummies(df, columns=['PERSONNEL'])

    print("Before OHE:", df_out['PERSONNEL'].unique())

    # Reset index before doing anything
    df_out = df_out.reset_index(drop=True)

    # Create dummy variables in a single clean step
    dummies = pd.get_dummies(
        df_out['PERSONNEL'].astype(str).str.strip(),
        prefix='PERSONNEL'
    )

    # Concatenate and then drop the original column
    df_out = pd.concat([df_out, dummies], axis=1).drop(columns=['PERSONNEL'])
    
    print("After OHE columns:", df_out.filter(like='PERSONNEL_').columns.tolist())
    print(df_out.filter(like='PERSONNEL_').tail(3))

    df_out['PLAY OF DRIVE NUM'] = -1
    for i in range(len(df)):
        if (df_out.iloc[i]['DN'] == 0):
            df_out.iloc[i, df_out.columns.get_loc('PLAY OF DRIVE NUM')] = 0
        else:
            df_out.iloc[i, df_out.columns.get_loc('PLAY OF DRIVE NUM')] = df_out.iloc[(i - 1), df_out.columns.get_loc('PLAY OF DRIVE NUM')] + 1

    # Loop through all columns that start with "PERSONNEL_"
    for col in df_out.columns:
        if col.startswith("PERSONNEL_"):
            df_out[f"{col} LAG 1"] = df_out[col].shift(1)
            df_out[f"{col} LAG 2"] = df_out[col].shift(2)

    df_out['2 MIN'] = df_out['2 MIN'].astype(str).str.strip()
    df_out['2 MIN OR NOT'] = -1
    df_out['2 MIN OR NOT'] = df_out['2 MIN'].apply(lambda x: 1 if x == "Y" else 0)

    df_out['OWN END'] = -1
    df_out['OWN END'] = df_out['YARD LN'].apply(lambda x: 1 if x < 0 else 0)

    df_out['OPP END'] = -1
    df_out['OPP END'] = df_out['YARD LN'].apply(lambda x: 1 if x >= 0 else 0)

    df_out['RED ZONE'] = -1
    df_out['RED ZONE'] = df_out['YARD LN'].apply(lambda x: 1 if (x <= 20 and x > 0) else 0)
    df_out['HALF_NUM'] = -1
    df_out['HALF_NUM'] = df_out['QTR'].apply(lambda x: 1 if x <= 2 else 2)

    df_out['TIME LEFT'] = -1
    df_out['TIME LEFT'] = df_out.apply(lambda row: row['TIME TO HALF'] if row['HALF_NUM'] == 2 else (row['TIME TO HALF'] + 900), axis=1)

    df_out['PPS NEEDED'] = -1
    df_out['PPS NEEDED'] = (df_out['SCORE DIFF']) * -1/df_out['TIME LEFT']

    df_out['WINNING'] = -1
    df_out['WINNING'] = df_out['SCORE DIFF'].apply(lambda x: 1 if x < 0 else 0)

    df_out['DN X DIST'] = -1
    df_out['DN X DIST'] = df_out['DN'] * df_out['DIST']

    df_out['PREV PLAY PASS OR NOT'] = -1
    df_out['PREV PLAY PASS OR NOT'] = df_out['PLAY TYPE LAG 1'].apply(lambda x: 1 if x == "Pass" else 0)

    df_out['SCORE DIFF ^2'] = df_out['SCORE DIFF'] * df_out['SCORE DIFF']
    df_out['SCORE DIFF ^2'] = df_out['WINNING'].apply(lambda x: -x if x > 0 else x)

    df_out['SCORE DIFF x TIME LEFT'] = df_out['SCORE DIFF'] * df_out['TIME LEFT']

    df_out['SCORE DIFF x DN'] = df_out['SCORE DIFF'] * df_out['DN']

    df_out['SCORE DIFF / 7'] = df_out['SCORE DIFF'] / 7
    df_out['TIME LEFT * SCORE DIFF / 7'] = df_out['TIME LEFT'] * df_out['SCORE DIFF'] / 7

    df_out['SCORE DIFF x QTR'] = df_out['SCORE DIFF'] * df_out['QTR']

    df_out['YARDS TO TD'] = df_out['YARD LN'].apply(lambda x: x + 100 if x < 0 else x)
    df_out['YARDS TO TD * SCORE DIFF / 7'] = df_out['YARDS TO TD'] * df_out['SCORE DIFF'] / 7

    # drop the first 2 plays (because of lag features)
    df_out = df_out.iloc[2:].reset_index(drop=True)

    return df_out

def evaluate_feature_subset(features, X_train_full, y_train, X_test_full, y_test, model_type='top1'):
    selected_features = [f for i, f in enumerate(X_train_full.columns) if features[i] == 1]
    
    if not selected_features:
        return 0.0  # Avoid empty feature sets

    X_train = X_train_full[selected_features]
    X_test = X_test_full[selected_features]

    preprocessor = ColumnTransformer(
        transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), selected_features)]
    )

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=45))
    ])
    
    pipeline.fit(X_train, y_train)

    if model_type == 'top1':
        y_pred = pipeline.predict(X_test)
        return accuracy_score(y_test, y_pred)

    elif model_type == 'top2':
        y_proba = pipeline.predict_proba(X_test)
        classes = pipeline.named_steps['classifier'].classes_
        top2_idx = np.argsort(y_proba, axis=1)[:, -2:]
        top2_preds = np.array([[classes[i] for i in row] for row in top2_idx])
        correct_top2 = [y in preds for y, preds in zip(y_test, top2_preds)]
        return np.mean(correct_top2)

    else:
        raise ValueError("model_type must be 'top1' or 'top2'")
    
def evaluate_feature_subset_single(
    features,                  # list of column NAMES 
    X_train, y_train, 
    X_test,  y_test,
    model_type='top1',         # 'top1' or 'top2'
    random_state=45,
    save_name=None):
    if (model_type == 'top1'):
        categorical_transformer = OneHotEncoder(handle_unknown='ignore')

        preprocessor = ColumnTransformer(
            transformers=[('cat', categorical_transformer, features)]
        )

        # Create a Random Forest pipeline
        rf_pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', RandomForestClassifier(random_state=45))
        ])

        # Train the model
        rf_pipeline.fit(X_train, y_train)

        # Predict and evaluate
        y_pred = rf_pipeline.predict(X_test)
        print(classification_report(y_test, y_pred))

        if save_name:
            joblib.dump(rf_pipeline, f"{save_name}.pkl")

        return rf_pipeline

    elif (model_type == 'top2'):
        # Rebuild preprocessor and pipeline with top-2 selected features
        categorical_transformer = OneHotEncoder(handle_unknown='ignore')

        preprocessor_top2 = ColumnTransformer(
            transformers=[('cat', categorical_transformer, features)]
        )

        rf_pipeline_top2 = Pipeline(steps=[
            ('preprocessor', preprocessor_top2),
            ('classifier', RandomForestClassifier(random_state=45))
        ])

        # Fit on top-2 feature subset
        rf_pipeline_top2.fit(X_train, y_train)
        
        y_proba = rf_pipeline_top2.predict_proba(X_test)

        # Get class labels (in the same order as columns of y_proba)
        classes = rf_pipeline_top2.named_steps['classifier'].classes_

        # For each sample, get indices of top 2 probs
        top2_idx = np.argsort(y_proba, axis=1)[:, -2:]

        # Convert indices to class labels
        top2_preds = np.array([[classes[i] for i in row] for row in top2_idx])

        # y_test must be a numpy array or series of actual labels
        correct_top2 = [true in preds for true, preds in zip(y_test, top2_preds)]
        top2_accuracy = np.mean(correct_top2)
        print(f"Top-2 Accuracy: {top2_accuracy:.2f}")

        y_test_array = np.array(y_test)

        # Create 'soft' top-2 predictions
        soft_top2_preds = []
        for i, row in enumerate(top2_preds):
            true = y_test_array[i]
            if true in row:
                soft_top2_preds.append(true)  # Treat as correct if in top-2
            else:
                soft_top2_preds.append(row[1])  # Use 2nd best prediction

        # Print full classification report
        print(classification_report(y_test_array, soft_top2_preds))

        if save_name:
            joblib.dump(rf_pipeline_top2, f"{save_name}.pkl")

        return rf_pipeline_top2
    
def genetic_algorithm(X_train, y_train, X_test, y_test, n_generations=30, pop_size=50, mutation_rate=0.15, model_type='top1'):
    n_features = X_train.shape[1]
    population = [np.random.randint(0, 2, size=n_features).tolist() for _ in range(pop_size)]

    for generation in range(n_generations):
        scores = [evaluate_feature_subset(ind, X_train, y_train, X_test, y_test, model_type) for ind in population]
        print(f"Generation {generation}: Best score = {max(scores):.4f}")

        # Select top 50%
        sorted_pop = [x for _, x in sorted(zip(scores, population), reverse=True)]
        parents = sorted_pop[:pop_size // 2]

        # Crossover
        offspring = []
        while len(offspring) < pop_size - len(parents):
            p1, p2 = random.sample(parents, 2)
            cut = random.randint(1, n_features - 1)
            child = p1[:cut] + p2[cut:]
            offspring.append(child)

        # Mutation
        for child in offspring:
            if random.random() < mutation_rate:
                idx = random.randint(0, n_features - 1)
                child[idx] = 1 - child[idx]

        population = parents + offspring

    # Return best feature subset
    final_scores = [evaluate_feature_subset(ind, X_train, y_train, X_test, y_test, model_type) for ind in population]
    best_idx = np.argmax(final_scores)
    best_features = [f for i, f in enumerate(X_train.columns) if population[best_idx][i] == 1]
    
    return best_features 

def split_train(df, location, inputs, target):
    split_index = int(len(df) * location)
    train = df.iloc[:split_index]
    test = df.iloc[split_index:]

    X_train = train[inputs]
    X_test = test[inputs]
    y_train = train[target]
    y_test = test[target]

    return X_train, X_test, y_train, y_test

def rf_input(X_train, X_test, y_train, y_test, features):
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer(
        transformers=[('cat', categorical_transformer, features)]
    )

    # Create a Random Forest pipeline
    rf_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=45))
    ])

    # Train the model
    rf_pipeline.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = rf_pipeline.predict(X_test)
    print(classification_report(y_test, y_pred))

# TEMP

# Copy df, remove weird plays, create features
def pre_split_prep(df):
    df = df.copy()
    df = df_trim(df)
    df = create_features(df)
    return df

# Manually do df.columns and create a list df_input_cols with the appropriate valid columns, create df_target_col = 'OFF FORM'

# 
def get_best_features(df, df_input_cols, df_target_col):
    df = df.copy()
    X_train_temp, X_test_temp, y_train_temp, y_test_temp = split_train(
            df, 0.7, df_input_cols, df_target_col
        )
    best_features_top1 = genetic_algorithm(
        X_train_temp, y_train_temp, X_test_temp, y_test_temp,
        n_generations=15, pop_size=50,
        mutation_rate=0.15, model_type='top1'
    )

    best_features_top2 = genetic_algorithm(
        X_train_temp, y_train_temp, X_test_temp, y_test_temp,
        n_generations=15, pop_size=50,
        mutation_rate=0.15, model_type='top2'
    )

    evaluate_feature_subset_single(best_features_top1, X_train_temp, y_train_temp, 
                        X_test_temp, y_test_temp, model_type='top1')

    evaluate_feature_subset_single(best_features_top2, X_train_temp, y_train_temp, 
                        X_test_temp, y_test_temp, model_type='top2')
    
    return best_features_top1, best_features_top2

def edit_previous_play(df, values=None, rewrite='y'):
    df_new = df.copy()
    if (rewrite =='y'):
        if (values is None):
            return "Rewriting requires a list of values"
        df_new.loc[len(df_new) - 1] = values

    elif (rewrite =='n'):
        df_new = df_new.drop(index=len(df_new) - 1)

    else:
        return "Must set rewrite='y' or 'n'"

    return df_new

def create_possible_rows(df_historical, values):
    # possibilities = []

    # Verify 'PERSONNEL' column exists
    if 'PERSONNEL' not in df_historical.columns:
        raise KeyError("The dataframe must contain a 'PERSONNEL' column.")

    # Get all unique personnel options
    options = df_historical['PERSONNEL'].dropna().unique().tolist()
    possible_rows = []

    for option in options:
        new_row = values.copy()
        new_row['PERSONNEL'] = option

        # Append as a clean dict matching df_historical.columns
        possible_rows.append({
            col: new_row.get(col, None) for col in df_historical.columns
        })
        # print(new_row)

    return possible_rows    

# Historical will have input cols but also values that need to be entered after the play such as GN/LS, PLAY TYPE (enter 'Pass' or other)
# as well as OFF FORM and PERSONNEL, GN/LS, PLAY TYPE, OFF FORM, PERSONNEL will all be known after the play, for now logged as empty values
# this means that they should enter the pre snap values, make copy(), do the pre_split_prep() on copy, run model,
# then get outputs for use, then from there use edit previous to input those things after the play


# def prompt_new_play_inputs(df_history):
#     """
#     Prompt the user for new play information, using defaults from the previous row when available.
#     Returns a dictionary of raw input values ready for create_possible_rows().
#     """

#     if df_history.empty:
#         prev_qtr, prev_own, prev_opp = 1, 0, 0
#         next_play = 1
#     else:
#         prev_row = df_history.iloc[-1]
#         prev_qtr = prev_row.get('QTR', 1)
#         prev_own = prev_row.get('OWN SCORE', 0)
#         prev_opp = prev_row.get('OPP SCORE', 0)
#         next_play = prev_row.get('PLAY #', 0) + 1

#     print("\n--- Enter new play details ---")
#     print("(Press Enter to keep default values in brackets)\n")

#     # Play number
#     play_num = simpledialog.askinteger("Play #", f"Play # [default={next_play}]. If plays are not consecutive the actual number doesn't matter but you should add at least 5 to the play # default: ")
#     if play_num is None:
#         play_num = next_play  # user pressed Enter or closed dialog

#     # Quarter
#     qtr = simpledialog.askinteger("Quarter", f"Quarter [default={prev_qtr}]: ")
#     if qtr is None:
#         qtr = prev_qtr  # user pressed Enter or closed dialog

#     time_input = simpledialog.askstring("Time Remaining", "Time on clock (MM:SS): ")

#     mm, ss = time_input.split(":")
#     mm = int(mm)
#     ss = int(ss)

#     # Adjust based on quarter (add 15 minutes for Q1 or Q3)
#     if qtr in [1, 3]:
#         mm = mm + 15

#     # Rebuild the string in MM:SS format (ensure 2-digit padding)
#     time_to_half = f"{mm:02d}:{ss:02d}"

#     # Down
#     dn = simpledialog.askinteger("Down", "Down (1-4 or 0 if start of posession): ")

#     # Distance
#     dist = simpledialog.askinteger("Yards to go", "Distance to first down (yards): ")

#     # Yard line
#     yard_ln = simpledialog.askinteger("Yard Line", "Yard line (-1 to -49 for own end, 50 to 0 for opponent end): ")

#     # Hash
#     hash_opt = simpledialog.askstring("Hash", "Hash (L / M / R): ").upper()
#     while hash_opt not in ["L", "M", "R"]:
#         messagebox.showwarning("Invalid", "Please enter L, M, or R.")
#         hash_opt = simpledialog.askstring("Hash", "Hash (L / M / R):").upper()

#     # Scores (default to previous)
#     own_score = simpledialog.askinteger("Own Score", f"Own score [default={prev_own}]: ")
#     if own_score is None:
#         own_score = prev_own  # user pressed Enter or closed dialog

#     opp_score = simpledialog.askinteger("Opponent Score", f"Opponent score [default={prev_opp}]: ")
#     if opp_score is None:
#         opp_score = prev_opp  # user pressed Enter or closed dialog

#     # Calculate 2-minute warning flag
#     two_min = 'Y' if (qtr in [2, 4] and time_to_half <= 120) else 0


#     # Build dict
#     play_dict = {
#         "PLAY #": play_num,
#         "QTR": qtr,
#         "TIME TO HALF": time_to_half,
#         "DN": dn,
#         "DIST": dist,
#         "YARD LN": yard_ln,
#         "HASH": hash_opt,
#         "PLAY TYPE": None,
#         "RESULT": None,
#         "OWN SCORE": own_score,
#         "OPP SCORE": opp_score,
#         "GN/LS": None,
#         "2 MIN": two_min,
#         "PERSONNEL": None,
#         "OFF FORM": None
#     }

#     messagebox.showinfo("Inputs Captured", f"Play Info: {play_dict}")

#     return play_dict




def prompt_new_play_inputs(
    df_history,
    play_num,
    qtr,
    time_to_half,
    dn,
    dist,
    yard_ln,
    hash_opt,
    own_score,
    opp_score,
    two_min_flag
):
    """
    Build and return a play_dict using the provided field values.
    No globals are referenced here â€” all inputs are passed from the frontend.
    """

    play_dict = {
        "PLAY #": play_num,
        "QTR": qtr,
        "TIME TO HALF": time_to_half,
        "DN": dn,
        "DIST": dist,
        "YARD LN": yard_ln,
        "HASH": hash_opt,
        "PLAY TYPE": None,        # set later in Enter Actual Results
        "RESULT": None,           # set later
        "OWN SCORE": own_score,
        "OPP SCORE": opp_score,
        "GN/LS": None,            # set after play
        "2 MIN": two_min_flag,
        "PERSONNEL": None,
        "OFF FORM": None,
    }

    messagebox.showinfo("Inputs Captured", f"Play Info: {play_dict}")
    return play_dict




def predict_top2(model, X):
    y_proba = model.predict_proba(X)
    top2_idx = np.argsort(y_proba, axis=1)[:, -2:]
    classes = model.named_steps['classifier'].classes_
    return np.array([[classes[i] for i in row] for row in top2_idx])

# For top1 prediction simply do the following: y_pred = model_top1.predict(new_X)

# I think there will be extra columns that need to be removed at the X = ... section, must adress later
# I think this is solved


def display_forms(df_hist, current_play, model):
    predictions = {}

    possible_rows = create_possible_rows(df_hist, current_play)

    for row in possible_rows:

        # print("first check: did each possibility get appended as the last row, the following should be the new row last")
        # print(df_hist["PERSONNEL"].tail(5))
        
        # Do preprocessing
        df_temp = df_trim(df_hist)

        # Create temp df: existing + new row
        df_temp = pd.concat([df_temp, pd.DataFrame([row])], ignore_index=True)

        # Do feature engineering
        df_temp = create_features(df_temp)

        # print("check after pre_split_prep: next prints are the last 5 columns of each personnel OHE column in order 10, 11, 12, 11T, 12T")
        # print(df_temp["PERSONNEL_10"].tail(5))
        # print(df_temp["PERSONNEL_11"].tail(5))
        # print(df_temp["PERSONNEL_12"].tail(5))
        # print(df_temp["PERSONNEL_11T"].tail(5))
        # print(df_temp["PERSONNEL_12T"].tail(5))

        # Remove the target variable from the set to select the inputs only
        X = df_temp.iloc[[-1]].drop(columns=['OFF FORM'])

        # Get top 2 most likely formations for this personnel group
        top2_preds = predict_top2(model, X)

        # Store in a dictionary using the personnel group as the key for ease of use with GUI later
        personnel_group = row['PERSONNEL']
        predictions[personnel_group] = top2_preds.flatten().tolist()

    return predictions

# def enter_results(df, rewrite='y'):
#     df_new = df.copy()
#     last_idx = len(df_new) - 1

#     if (rewrite == 'n'):
#         df_new = edit_previous_play(df_new, rewrite='n')
#         return df_new

#     elif rewrite == 'y':
#         messagebox.showinfo("Enter Results", "This will later collect the true formation, result, etc.")
#         play_type = simpledialog.askstring("Play Type", f"Play type ('Run' or 'Pass'): ").strip().title()
#         result = simpledialog.askstring("Result of Play", f"Result of play('Rush', 'Complete', 'Incomplete', 'Sack', 'Interception', 'Timeout', 'Penalty') must be sure of no typos: ").strip().title()
#         gn_ls = simpledialog.askinteger("Yards Gained or Lost", f"Gain or loss of yards: ")
#         personnel = simpledialog.askstring("True Personnel", f"Personnel group (ex: '10', '12', '12T'): ").upper().strip()
#         off_form = simpledialog.askstring("True Formation", f"Offensive formation (ex: 'TRIO', 'DUTCH Y OFF') MUST HAVE EXACT SPELLING!!!").upper().strip()
        
#         updates = {
#         'PLAY TYPE': play_type,
#         'RESULT': result,
#         'GN/LS': gn_ls,
#         'PERSONNEL': personnel,
#         'OFF FORM': off_form
#         }
        
#         for col, val in updates.items():
#             df_new.at[last_idx, col] = val

#         return df_new

#     else: 
#         return "Must set rewrite='y' or 'n'"

def enter_results(df, results_dict):
    """
    Update the last play in df_history with the entered results.
    results_dict should include:
        'PLAY TYPE', 'RESULT', 'GN/LS', 'PERSONNEL', 'OFF FORM'
    """
    df_new = df.copy()
    last_idx = len(df_new) - 1

    # Loop through provided updates and apply them
    for col, val in results_dict.items():
        if col in df_new.columns:
            df_new.at[last_idx, col] = val

    messagebox.showinfo("Results Recorded", f"Updated play #{df_new.at[last_idx, 'PLAY #']} with results.")
    return df_new




